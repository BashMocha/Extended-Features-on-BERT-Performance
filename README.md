## Extended Features on BERT Performance

<b>Official implementation of the [SIU 2025](https://www.ieee.org.tr/33-ieee-sinyal-isleme-ve-iletisim-uygulamalari-kurultayi-siu/) paper.</b>

Emirhan BalcÄ±*, Esra SaraÃ§

## Abstract
In this study, the effects of categorical and numerical additional features obtained from Twitter posts on depression detection were investigated. Depression detection performances of the BERT large language model and SVM classifier were compared on the dataset balanced with the oversampling method. The effects of two different feature addition methods, Unimodal and Concat, were evaluated on the BERT model. The results show that oversampling improves the performance of the BERT classifier, but feature addition methods do not provide a significant improvement in the model performance. The findings of the experiments reveal the success of the BERT model in the field of classification and that it does not require additional features for the detection of depression. It is believed that this study will guide research in the field of depression detection and help researchers identify more effective areas of study.

[Code](https://github.com/BashMocha/Extended-Features-on-BERT-Performance/tree/master/notebooks) | [Paper]() | [Data](https://github.com/BashMocha/Extended-Features-on-BERT-Performance/tree/master/data)

## Updates

25/06/2025: We release the utilized dataset and the source code.

13/05/2025: The study is accepted by SIU 2025! ðŸŽ‰

09/02/2025: The paper is submitted to the symposium.

## Citation

If you find the dataset or code useful, please cite:

```bibtex
@inproceedings{balci_extended_2025,
	title = {Effects of Extended Features on BERT Performance: Depression Detection},
	booktitle = {2025 33rd IEEE Conference on Signal Processing and Communications Applications (SIU2025},
	author = {BalcÄ±, Emirhan and SaraÃ§, Esra},
	year = {2025},
}
```

## License

MIT License

<hr>

Feel free to [contact](mailto:emirbalci360@gmail.com) for any questions.
